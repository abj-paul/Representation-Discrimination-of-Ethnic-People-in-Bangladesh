{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chdELC97b7ha"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO-y1r5XB96m",
        "outputId": "a43c0d7e-5229-488f-8be6-0d31ff4cd520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "data_batches = joblib.load(\"/content/drive/MyDrive/data_batches.joblib\")\n",
        "data_batches[0][:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZbL2tj5xlqJ",
        "outputId": "64f3306b-bbaa-4d7b-8a7a-f2a6683a1329"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['বোরকা',\n",
              "  'নিষিদ্ধ',\n",
              "  'পথ',\n",
              "  'ধাপ',\n",
              "  'এগি',\n",
              "  'শ্রীলঙ্কা',\n",
              "  'দেশ',\n",
              "  'মন্ত্রিসভা',\n",
              "  'সংক্রান্ত',\n",
              "  'প্রস্তাব',\n",
              "  'অনুমোদন',\n",
              "  'দেশ',\n",
              "  'জননিরাপত্তাবিষয়ক',\n",
              "  'মন্ত্রী',\n",
              "  'শরৎ',\n",
              "  'বীরসেকেরা',\n",
              "  'তথ্য',\n",
              "  'জানিয়েছেন',\n",
              "  'কাতারভিত্তিক',\n",
              "  'সংবাদমাধ্যম',\n",
              "  'জাজি',\n",
              "  'বুধবার',\n",
              "  'প্রতিবেদন',\n",
              "  'জানা',\n",
              "  'শ্রীলঙ্কা',\n",
              "  'মন্ত্রিসভা',\n",
              "  'সাপ্তাহিক',\n",
              "  'বৈঠক',\n",
              "  'অনুষ্ঠিত',\n",
              "  'হ',\n",
              "  'গতকাল',\n",
              "  'বৈঠ',\n",
              "  'বোরকা',\n",
              "  'নিষিদ্ধ',\n",
              "  'প্রস্তাব',\n",
              "  'সম্মত',\n",
              "  'হ',\n",
              "  'জননিরাপত্তাবিষয়ক',\n",
              "  'মন্ত্রী',\n",
              "  'শরৎ',\n",
              "  'বীরসেকেরা',\n",
              "  'তথ্য',\n",
              "  'জানি',\n",
              "  'ফেসবুক',\n",
              "  'প',\n",
              "  'পোস্ট',\n",
              "  'প্রস্তাব',\n",
              "  'অ্যাটর্নি',\n",
              "  'জেনারেল',\n",
              "  'পাঠানো',\n",
              "  'এরপর',\n",
              "  'পার্লামেন্ট',\n",
              "  'পাস',\n",
              "  'আইন',\n",
              "  'পরিণত',\n",
              "  'শ্রীলঙ্কা',\n",
              "  'পার্লামেন্ট',\n",
              "  'সরকারি',\n",
              "  'দল',\n",
              "  'সংখ্যাগরিষ্ঠতা',\n",
              "  'র',\n",
              "  'সহজ',\n",
              "  'আইন',\n",
              "  'হিস',\n",
              "  'পাস'],\n",
              " ['মার্চ',\n",
              "  'সংবাদ',\n",
              "  'সম্মেলন',\n",
              "  'শ্রীলঙ্কা',\n",
              "  'জননিরাপত্তাবিষয়ক',\n",
              "  'মন্ত্রী',\n",
              "  'বোরকা',\n",
              "  'নিষিদ্ধ',\n",
              "  'উদ্যোগ',\n",
              "  'নেওয়ার',\n",
              "  'কথা',\n",
              "  'জানিয়েছিলেন',\n",
              "  'সম',\n",
              "  'জানান',\n",
              "  'ধর্মী',\n",
              "  'উগ্রবাদ',\n",
              "  'রোধ',\n",
              "  'জাতী',\n",
              "  'নিরাপত্তা',\n",
              "  'রক্ষ',\n",
              "  'স্বার্থ',\n",
              "  'উদ্যোগ',\n",
              "  'আইন',\n",
              "  'পাস',\n",
              "  'মুসলিম',\n",
              "  'নারী',\n",
              "  'জনসমাগমস্থলে',\n",
              "  'পুরো',\n",
              "  'মুখ',\n",
              "  'ঢ',\n",
              "  'পার',\n",
              "  'শ্রীলঙ্কা',\n",
              "  'সরকার',\n",
              "  'উদ্যোগ',\n",
              "  'সমালোচনা',\n",
              "  'ধর্মী',\n",
              "  'স্বাধীনতাবিষয়ক',\n",
              "  'জাতিসংঘ',\n",
              "  'দূত',\n",
              "  'আহমেদ',\n",
              "  'শাহেদ',\n",
              "  'টুইট',\n",
              "  'বার্তা',\n",
              "  'আন্তর্জাতিক',\n",
              "  'আইন',\n",
              "  'ধর্মী',\n",
              "  'মতামত',\n",
              "  'প্রকাশ',\n",
              "  'স্বাধীনত',\n",
              "  'পরিপন্থী',\n",
              "  'কলম্বো',\n",
              "  'পাকিস্তান',\n",
              "  'হাইকমিশন',\n",
              "  'সাদ',\n",
              "  'খাট্টাক',\n",
              "  'বল',\n",
              "  'আইন',\n",
              "  'দেশটি',\n",
              "  'মুসলিম',\n",
              "  'মূল্যবোধে',\n",
              "  'আঘাত',\n",
              "  'আগ',\n",
              "  'বোরকা',\n",
              "  'নিষিদ্ধ',\n",
              "  'উদ্যোগ',\n",
              "  'নি',\n",
              "  'শ্রীলঙ্কা',\n",
              "  'সরক',\n",
              "  '২০১৯',\n",
              "  'সাল',\n",
              "  'দেশ',\n",
              "  'কয়েক',\n",
              "  'গির্জা',\n",
              "  'সিরিজ',\n",
              "  'বোমা',\n",
              "  'হামলা',\n",
              "  '২৫০',\n",
              "  'নিহত',\n",
              "  'হওয়া',\n",
              "  'ঘটনায়',\n",
              "  'মুসলিম',\n",
              "  'নারী',\n",
              "  'মুখঢাকা',\n",
              "  'বোরকা',\n",
              "  'পরিধান',\n",
              "  'সাময়িক',\n",
              "  'নিষিদ্ধ',\n",
              "  'কর',\n",
              "  'দেশ',\n",
              "  'শ্রীলঙ্কা',\n",
              "  'লাখ',\n",
              "  'জনসংখ্য',\n",
              "  'শতাংশ',\n",
              "  'মুসলিম',\n",
              "  'শতাংশ',\n",
              "  'বৌদ্ধ',\n",
              "  'শতাংশ',\n",
              "  'তামিল',\n",
              "  'তামিল',\n",
              "  'অধিকাংশ',\n",
              "  'সনাতনধর্মাবলম্বী']]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(data_batches)"
      ],
      "metadata": {
        "id": "i05p99RxVkJD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-rw2feBRDNI"
      },
      "source": [
        "### Batch Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ITe1pZiQLJv9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the Word2Vec model using PyTorch\n",
        "class Word2VecPytorch(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2VecPytorch, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_words):\n",
        "        embeds = self.embeddings(input_words)\n",
        "        output = self.linear(embeds)\n",
        "        return output\n",
        "\n",
        "def save_checkpoint(model, epoch, optimizer, loss):\n",
        "    model_dir = \"checkpoints\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    model_path = os.path.join(model_dir, f\"word2vec_model_epoch{epoch}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, model_path)\n",
        "    print(f\"Checkpoint saved: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_sentences = []\n",
        "for batch in tqdm(data_batches, desc=\"Flattening Batches\", unit=\"batch\"):\n",
        "    for sentence in batch:\n",
        "        processed_sentences.append(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O62RD8618JnA",
        "outputId": "767f390c-27c3-41cc-eae6-3f9a985a61a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Flattening Batches: 100%|██████████| 4/4 [00:00<00:00, 105.29batch/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "words = []\n",
        "for sentence in tqdm(processed_sentences, desc=f\"Counting \", unit=\"sentence\"):\n",
        "  for word in sentence:\n",
        "    words.append(word)\n",
        "word_counter = Counter(words)\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(word_counter.items())}\n",
        "vocab_size = len(vocab)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfr7DPCs_9ox",
        "outputId": "380293c7-75cc-49ef-f1e5-1d94301d16f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Counting : 100%|██████████| 400000/400000 [00:02<00:00, 184219.05sentence/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "316005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[\"টাকা\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk58X4caV041",
        "outputId": "203bc575-1bdd-42c3-a409-2666b0ad6997"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 400\n",
        "\n",
        "# Initialize Word2Vec model with PyTorch\n",
        "model = Word2VecPytorch(vocab_size, embedding_dim)\n",
        "model = model.cuda()  # Move model to GPU\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "x10p7nZE4sL3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[\"জরুরিভাবে\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cdhS6JOfcFo",
        "outputId": "cd43f553-c821-499a-a8bb-1c382fabc2e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13016"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch, data_batch in enumerate(data_batches):\n",
        "    total_loss = 0\n",
        "    for sentence in tqdm(data_batch, desc=f\"Epoch {epoch + 1}/{num_batches}\", unit=\"sentence\"):\n",
        "        for target_word in tqdm(sentence, desc=\"Processing Words\", unit=\"word\"):\n",
        "            # Iterate over context words\n",
        "            for context_word in sentence:\n",
        "                if context_word != target_word:\n",
        "                    # Convert words to indices\n",
        "                    target_idx = torch.tensor([vocab[target_word]], dtype=torch.long).cuda()\n",
        "                    context_idx = torch.tensor([vocab[context_word]], dtype=torch.long).cuda()\n",
        "\n",
        "                    # Zero the gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # Forward pass\n",
        "                    output = model(context_idx)\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = criterion(output, target_idx)\n",
        "\n",
        "                    # Backward pass\n",
        "                    loss.backward()\n",
        "\n",
        "                    # Update weights\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # Accumulate loss\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "    # Print epoch loss\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    save_checkpoint(model, epoch, optimizer, total_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mocaQjEV4C9Z",
        "outputId": "72c8c44b-5ef2-42e2-d867-0067618464eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/4:   0%|          | 0/100000 [00:00<?, ?sentence/s]\n",
            "Processing Words:   0%|          | 0/65 [00:00<?, ?word/s]\u001b[A\n",
            "Processing Words:   2%|▏         | 1/65 [00:00<00:18,  3.39word/s]\u001b[A\n",
            "Processing Words:   3%|▎         | 2/65 [00:00<00:18,  3.39word/s]\u001b[A\n",
            "Processing Words:   5%|▍         | 3/65 [00:00<00:18,  3.37word/s]\u001b[A\n",
            "Processing Words:   6%|▌         | 4/65 [00:01<00:17,  3.39word/s]\u001b[A\n",
            "Processing Words:   8%|▊         | 5/65 [00:01<00:17,  3.41word/s]\u001b[A\n",
            "Processing Words:   9%|▉         | 6/65 [00:01<00:17,  3.45word/s]\u001b[A\n",
            "Processing Words:  11%|█         | 7/65 [00:02<00:16,  3.45word/s]\u001b[A\n",
            "Processing Words:  12%|█▏        | 8/65 [00:02<00:16,  3.46word/s]\u001b[A\n",
            "Processing Words:  14%|█▍        | 9/65 [00:02<00:16,  3.45word/s]\u001b[A\n",
            "Processing Words:  15%|█▌        | 10/65 [00:02<00:15,  3.48word/s]\u001b[A\n",
            "Processing Words:  17%|█▋        | 11/65 [00:03<00:15,  3.47word/s]\u001b[A\n",
            "Processing Words:  18%|█▊        | 12/65 [00:03<00:15,  3.47word/s]\u001b[A\n",
            "Processing Words:  20%|██        | 13/65 [00:03<00:14,  3.48word/s]\u001b[A\n",
            "Processing Words:  22%|██▏       | 14/65 [00:04<00:14,  3.47word/s]\u001b[A\n",
            "Processing Words:  23%|██▎       | 15/65 [00:04<00:14,  3.48word/s]\u001b[A\n",
            "Processing Words:  25%|██▍       | 16/65 [00:04<00:14,  3.48word/s]\u001b[A\n",
            "Processing Words:  26%|██▌       | 17/65 [00:04<00:13,  3.48word/s]\u001b[A\n",
            "Processing Words:  28%|██▊       | 18/65 [00:05<00:13,  3.46word/s]\u001b[A\n",
            "Processing Words:  29%|██▉       | 19/65 [00:05<00:13,  3.45word/s]\u001b[A\n",
            "Processing Words:  31%|███       | 20/65 [00:05<00:13,  3.45word/s]\u001b[A\n",
            "Processing Words:  32%|███▏      | 21/65 [00:06<00:12,  3.44word/s]\u001b[A\n",
            "Processing Words:  34%|███▍      | 22/65 [00:06<00:12,  3.42word/s]\u001b[A\n",
            "Processing Words:  35%|███▌      | 23/65 [00:06<00:12,  3.42word/s]\u001b[A\n",
            "Processing Words:  37%|███▋      | 24/65 [00:06<00:12,  3.41word/s]\u001b[A\n",
            "Processing Words:  38%|███▊      | 25/65 [00:07<00:11,  3.45word/s]\u001b[A\n",
            "Processing Words:  40%|████      | 26/65 [00:07<00:11,  3.46word/s]\u001b[A\n",
            "Processing Words:  42%|████▏     | 27/65 [00:07<00:11,  3.44word/s]\u001b[A\n",
            "Processing Words:  43%|████▎     | 28/65 [00:08<00:10,  3.43word/s]\u001b[A\n",
            "Processing Words:  45%|████▍     | 29/65 [00:08<00:10,  3.43word/s]\u001b[A\n",
            "Processing Words:  46%|████▌     | 30/65 [00:08<00:10,  3.44word/s]\u001b[A\n",
            "Processing Words:  48%|████▊     | 31/65 [00:09<00:09,  3.43word/s]\u001b[A\n",
            "Processing Words:  49%|████▉     | 32/65 [00:09<00:09,  3.43word/s]\u001b[A\n",
            "Processing Words:  51%|█████     | 33/65 [00:09<00:09,  3.43word/s]\u001b[A\n",
            "Processing Words:  52%|█████▏    | 34/65 [00:09<00:08,  3.45word/s]\u001b[A\n",
            "Processing Words:  54%|█████▍    | 35/65 [00:10<00:08,  3.47word/s]\u001b[A\n",
            "Processing Words:  55%|█████▌    | 36/65 [00:10<00:08,  3.46word/s]\u001b[A\n",
            "Processing Words:  57%|█████▋    | 37/65 [00:10<00:08,  3.47word/s]\u001b[A\n",
            "Processing Words:  58%|█████▊    | 38/65 [00:11<00:07,  3.45word/s]\u001b[A\n",
            "Processing Words:  60%|██████    | 39/65 [00:11<00:07,  3.46word/s]\u001b[A\n",
            "Processing Words:  62%|██████▏   | 40/65 [00:11<00:07,  3.46word/s]\u001b[A\n",
            "Processing Words:  63%|██████▎   | 41/65 [00:11<00:06,  3.46word/s]\u001b[A\n",
            "Processing Words:  65%|██████▍   | 42/65 [00:12<00:06,  3.44word/s]\u001b[A\n",
            "Processing Words:  66%|██████▌   | 43/65 [00:12<00:06,  3.43word/s]\u001b[A\n",
            "Processing Words:  68%|██████▊   | 44/65 [00:12<00:06,  3.42word/s]\u001b[A\n",
            "Processing Words:  69%|██████▉   | 45/65 [00:13<00:05,  3.42word/s]\u001b[A\n",
            "Processing Words:  71%|███████   | 46/65 [00:13<00:05,  3.42word/s]\u001b[A\n",
            "Processing Words:  72%|███████▏  | 47/65 [00:13<00:05,  3.45word/s]\u001b[A\n",
            "Processing Words:  74%|███████▍  | 48/65 [00:13<00:04,  3.44word/s]\u001b[A\n",
            "Processing Words:  75%|███████▌  | 49/65 [00:14<00:04,  3.44word/s]\u001b[A\n",
            "Processing Words:  77%|███████▋  | 50/65 [00:14<00:04,  3.44word/s]\u001b[A\n",
            "Processing Words:  78%|███████▊  | 51/65 [00:14<00:04,  3.43word/s]\u001b[A\n",
            "Processing Words:  80%|████████  | 52/65 [00:15<00:03,  3.44word/s]\u001b[A\n",
            "Processing Words:  82%|████████▏ | 53/65 [00:15<00:03,  3.46word/s]\u001b[A\n",
            "Processing Words:  83%|████████▎ | 54/65 [00:15<00:03,  3.47word/s]\u001b[A\n",
            "Processing Words:  85%|████████▍ | 55/65 [00:15<00:02,  3.46word/s]\u001b[A\n",
            "Processing Words:  86%|████████▌ | 56/65 [00:16<00:02,  3.48word/s]\u001b[A\n",
            "Processing Words:  88%|████████▊ | 57/65 [00:16<00:02,  3.49word/s]\u001b[A\n",
            "Processing Words:  89%|████████▉ | 58/65 [00:16<00:02,  3.47word/s]\u001b[A\n",
            "Processing Words:  91%|█████████ | 59/65 [00:17<00:01,  3.46word/s]\u001b[A\n",
            "Processing Words:  92%|█████████▏| 60/65 [00:17<00:01,  3.42word/s]\u001b[A\n",
            "Processing Words:  94%|█████████▍| 61/65 [00:17<00:01,  3.42word/s]\u001b[A\n",
            "Processing Words:  95%|█████████▌| 62/65 [00:17<00:00,  3.42word/s]\u001b[A\n",
            "Processing Words:  97%|█████████▋| 63/65 [00:18<00:00,  3.44word/s]\u001b[A\n",
            "Processing Words:  98%|█████████▊| 64/65 [00:18<00:00,  3.43word/s]\u001b[A\n",
            "Processing Words: 100%|██████████| 65/65 [00:18<00:00,  3.44word/s]\n",
            "Epoch 1/4:   0%|          | 1/100000 [00:18<524:16:16, 18.87s/sentence]\n",
            "Processing Words:   0%|          | 0/101 [00:00<?, ?word/s]\u001b[A\n",
            "Processing Words:   1%|          | 1/101 [00:00<00:45,  2.22word/s]\u001b[A\n",
            "Processing Words:   2%|▏         | 2/101 [00:00<00:44,  2.21word/s]\u001b[A\n",
            "Processing Words:   3%|▎         | 3/101 [00:01<00:44,  2.21word/s]\u001b[A\n",
            "Processing Words:   4%|▍         | 4/101 [00:01<00:43,  2.23word/s]\u001b[A\n",
            "Processing Words:   5%|▍         | 5/101 [00:02<00:43,  2.22word/s]\u001b[A\n",
            "Processing Words:   6%|▌         | 6/101 [00:02<00:42,  2.21word/s]\u001b[A\n",
            "Processing Words:   7%|▋         | 7/101 [00:03<00:42,  2.21word/s]\u001b[A\n",
            "Processing Words:   8%|▊         | 8/101 [00:03<00:41,  2.22word/s]\u001b[A\n",
            "Processing Words:   9%|▉         | 9/101 [00:04<00:41,  2.24word/s]\u001b[A\n",
            "Processing Words:  10%|▉         | 10/101 [00:04<00:40,  2.22word/s]\u001b[A\n",
            "Processing Words:  11%|█         | 11/101 [00:04<00:40,  2.21word/s]\u001b[A\n",
            "Processing Words:  12%|█▏        | 12/101 [00:05<00:40,  2.21word/s]\u001b[A\n",
            "Processing Words:  13%|█▎        | 13/101 [00:05<00:39,  2.20word/s]\u001b[A\n",
            "Processing Words:  14%|█▍        | 14/101 [00:06<00:39,  2.20word/s]\u001b[A\n",
            "Processing Words:  15%|█▍        | 15/101 [00:06<00:38,  2.22word/s]\u001b[A\n",
            "Processing Words:  16%|█▌        | 16/101 [00:07<00:38,  2.21word/s]\u001b[A\n",
            "Processing Words:  17%|█▋        | 17/101 [00:07<00:38,  2.20word/s]\u001b[A\n",
            "Processing Words:  18%|█▊        | 18/101 [00:08<00:37,  2.20word/s]\u001b[A\n",
            "Processing Words:  19%|█▉        | 19/101 [00:08<00:37,  2.20word/s]\u001b[A\n",
            "Processing Words:  20%|█▉        | 20/101 [00:09<00:36,  2.20word/s]\u001b[A\n",
            "Processing Words:  21%|██        | 21/101 [00:09<00:36,  2.20word/s]\u001b[A\n",
            "Processing Words:  22%|██▏       | 22/101 [00:09<00:35,  2.22word/s]\u001b[A\n",
            "Processing Words:  23%|██▎       | 23/101 [00:10<00:34,  2.23word/s]\u001b[A\n",
            "Processing Words:  24%|██▍       | 24/101 [00:10<00:34,  2.22word/s]\u001b[A\n",
            "Processing Words:  25%|██▍       | 25/101 [00:11<00:34,  2.23word/s]\u001b[A\n",
            "Processing Words:  26%|██▌       | 26/101 [00:11<00:33,  2.23word/s]\u001b[A\n",
            "Processing Words:  27%|██▋       | 27/101 [00:12<00:33,  2.22word/s]\u001b[A\n",
            "Processing Words:  28%|██▊       | 28/101 [00:12<00:33,  2.21word/s]\u001b[A\n",
            "Processing Words:  29%|██▊       | 29/101 [00:13<00:32,  2.21word/s]\u001b[A\n",
            "Processing Words:  30%|██▉       | 30/101 [00:13<00:32,  2.21word/s]\u001b[A\n",
            "Processing Words:  31%|███       | 31/101 [00:14<00:31,  2.21word/s]\u001b[A\n",
            "Processing Words:  32%|███▏      | 32/101 [00:14<00:31,  2.22word/s]\u001b[A\n",
            "Processing Words:  33%|███▎      | 33/101 [00:14<00:30,  2.21word/s]\u001b[A\n",
            "Processing Words:  34%|███▎      | 34/101 [00:15<00:30,  2.23word/s]\u001b[A\n",
            "Processing Words:  35%|███▍      | 35/101 [00:15<00:29,  2.21word/s]\u001b[A\n",
            "Processing Words:  36%|███▌      | 36/101 [00:16<00:29,  2.22word/s]\u001b[A\n",
            "Processing Words:  37%|███▋      | 37/101 [00:16<00:28,  2.22word/s]\u001b[A\n",
            "Processing Words:  38%|███▊      | 38/101 [00:17<00:28,  2.21word/s]\u001b[A\n",
            "Processing Words:  39%|███▊      | 39/101 [00:17<00:28,  2.21word/s]\u001b[A\n",
            "Processing Words:  40%|███▉      | 40/101 [00:18<00:27,  2.19word/s]\u001b[A\n",
            "Processing Words:  41%|████      | 41/101 [00:18<00:27,  2.19word/s]\u001b[A\n",
            "Processing Words:  42%|████▏     | 42/101 [00:18<00:26,  2.20word/s]\u001b[A\n",
            "Processing Words:  43%|████▎     | 43/101 [00:19<00:26,  2.20word/s]\u001b[A\n",
            "Processing Words:  44%|████▎     | 44/101 [00:19<00:25,  2.20word/s]\u001b[A\n",
            "Processing Words:  45%|████▍     | 45/101 [00:20<00:25,  2.21word/s]\u001b[A\n",
            "Processing Words:  46%|████▌     | 46/101 [00:20<00:24,  2.23word/s]\u001b[A\n",
            "Processing Words:  47%|████▋     | 47/101 [00:21<00:24,  2.22word/s]\u001b[A\n",
            "Processing Words:  48%|████▊     | 48/101 [00:21<00:23,  2.22word/s]\u001b[A\n",
            "Processing Words:  49%|████▊     | 49/101 [00:22<00:23,  2.21word/s]\u001b[A\n",
            "Processing Words:  50%|████▉     | 50/101 [00:22<00:23,  2.21word/s]\u001b[A\n",
            "Processing Words:  50%|█████     | 51/101 [00:23<00:22,  2.20word/s]\u001b[A\n",
            "Processing Words:  51%|█████▏    | 52/101 [00:23<00:22,  2.20word/s]\u001b[A\n",
            "Processing Words:  52%|█████▏    | 53/101 [00:23<00:21,  2.20word/s]\u001b[A\n",
            "Processing Words:  53%|█████▎    | 54/101 [00:24<00:21,  2.20word/s]\u001b[A\n",
            "Processing Words:  54%|█████▍    | 55/101 [00:24<00:20,  2.20word/s]\u001b[A\n",
            "Processing Words:  55%|█████▌    | 56/101 [00:25<00:20,  2.20word/s]\u001b[A\n",
            "Processing Words:  56%|█████▋    | 57/101 [00:25<00:19,  2.21word/s]\u001b[A\n",
            "Processing Words:  57%|█████▋    | 58/101 [00:26<00:19,  2.21word/s]\u001b[A\n",
            "Processing Words:  58%|█████▊    | 59/101 [00:26<00:18,  2.23word/s]\u001b[A\n",
            "Processing Words:  59%|█████▉    | 60/101 [00:27<00:18,  2.22word/s]\u001b[A\n",
            "Processing Words:  60%|██████    | 61/101 [00:27<00:18,  2.21word/s]\u001b[A\n",
            "Processing Words:  61%|██████▏   | 62/101 [00:28<00:17,  2.21word/s]\u001b[A\n",
            "Processing Words:  62%|██████▏   | 63/101 [00:28<00:17,  2.22word/s]\u001b[A\n",
            "Processing Words:  63%|██████▎   | 64/101 [00:28<00:16,  2.23word/s]\u001b[A\n",
            "Processing Words:  64%|██████▍   | 65/101 [00:29<00:16,  2.24word/s]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the final trained model\n",
        "final_model_path = \"final_word2vec_model.pt\"\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"Final model saved: {final_model_path}\")\n"
      ],
      "metadata": {
        "id": "NBYkzvMI0ShC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "97xDs7fuWS1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}